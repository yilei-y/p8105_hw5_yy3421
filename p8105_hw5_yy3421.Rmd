---
title: "p8105_hw5_yy3421"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 10, 
  fig.height = 8,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

### Description of the raw data

```{r data import}
homicide_data_raw = read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown"))
```


```{r city_state var and cleaning}
homicide_data = 
  homicide_data_raw |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved")
  ) |> 
  filter(city_state != "Tulsa, AL") 
```

The raw dataset contains `r homicide_data_raw |> nrow()` observations and `r homicide_data_raw |> ncol()` variables. This dataset also contains demographic information of the victim for each case, like age, sex, and race, and case-specific information, like date, and location.

Through data cleaning, a `city_state` variable was first created to combine the information from city and state variable. Furthermore, a `resolution` variable was created to distinguish the final status of each case. One entry in Tulsa, AL was excluded through data cleaning as the location is not a major US city.

```{r total and unsolved cases}
city_homicide_df = 
  homicide_data %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```


The above code chunks summarizes the total number of homicides and the number of homicides that were solved.

## Prop test for Baltimore

Looking at cases located in Baltimore, MD, proportion test was used, complemented by `broom::tidy` function, to estimate the proportion and confidence level of unsolved homicides in that city. 

```{r Balt prop}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD")  |>  pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD")  |>  pull(hom_total)) 

broom::tidy(bmore_test)  |>  
  knitr::kable(digits = 3)
```

Furthermore, estimates and CIs for the proportion of unsolved homicides for each city are presented below using the `map` function.

```{r all city prop}
test_results = 
  city_homicide_df  |>  
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy))  |>  
  select(-prop_tests)  |>  
  unnest(tidy_tests)  |>  
  select(city_state, estimate, conf.low, conf.high)  |>  
  mutate(city_state = fct_reorder(city_state, estimate))
```

Using the data from the above data chunck, a point plot, along with error bar, is created to visualized the estimate of the proportion of unsolved homicides in each city. Chicago, IL has the highest estimate and Richmond, VA has the lowest.

```{r p1 plot}
test_results  |>  
  mutate(city_state = fct_reorder(city_state, estimate))  |>  
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Problem 2
```{r data cleaning and tidying}
files_collection = 
  tibble(
    file_name = list.files(path = "./data/problem_2"),
    file_path = paste(c("./data/problem_2/"), file_name, sep = "")
  ) |>
  mutate(
    data = map(file_path, read_csv)
  ) |> 
  separate(file_name, into = c("control_arm", "subject_ID"), sep = "_") |> 
  separate(subject_ID, into = c("subject_ID", "file_type"), sep = "\\.")|> 
  unnest(data) |> 
  pivot_longer(
    week_1:week_8,
    names_to = c("week"),
    values_to = c("data") 
  ) |> 
  separate(week, into = c("week", "time"), sep = "_")|> 
  select(
    -file_type, -file_path, -week
  ) |> 
  mutate(
    time = as.numeric(time)
  )
```

Making plot:
```{r plot}
files_collection |> 
  ggplot(aes(x = time, y = data, color = subject_ID))+
  geom_line()+
  facet_grid(~control_arm)
```

### Comment on differences between groups
Based on the spaghetti plot above, we can oberve that there is a higher average data for the experiment arm. Furthermore, for all participants in the experiment arm, there is a increasing trend in data as we move from week 2 to 8. However, most data among all participants in the control arm remains in the interval 0 to 2.5.

## Problem 3

```{r function for sim}
output = vector("list", 5000)

sim_t_test = function(mu, n = 30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = sigma),
  )
  
  sim_data |> 
    t.test(conf.level = 0.95) |> 
    broom::tidy() |> 
    select(
      estimate, p.value
    )
    
}

```

### Simulation for mu = 0
```{r mu = 0}
output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim_t_test(mu=0)
}

sim_results = bind_rows(output)

```

### Simulation for mu = 0:6
```{r mu simulation}
sim_results_df = 
  expand_grid(
    mu_size = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(mu_size, sim_t_test)
    )|> 
  unnest(estimate_df)
```

# the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis
```{r}
sim_results_df |> 
  group_by(mu_size) |> 
  filter(p.value < 0.05) |> 
  summarize(n_reject = n()) |> 
  mutate(
    n_reject = n_reject / 5000
  ) |> 
  ggplot(aes(x = mu_size, y = n_reject, color = mu_size))+
  geom_point()+
  geom_line()
```

Describe the association between effect size and power.
Based on the plot above, there is a decreasing trend in the proportion of times the null was rejected as we increase the mu size from 0 to 5. There is 0 times the null was rejected when mu equals to 6. Hence, as the effect size increases, the power of this study increases.

#second plot
```{r plot 2}
sim_results_df_mean_mu=
  sim_results_df|> 
  group_by(mu_size) |> 
  summarise(mean_estimate = mean(estimate))

sim_results_df |> 
  group_by(mu_size) |> 
  filter(p.value < 0.05) |> 
  summarize(mean_reject = mean(estimate)) |> 
  right_join(sim_results_df_mean_mu) |> 
  ggplot(aes(x = mu_size)) +
  geom_line(aes(y = mean_reject, color = "red"))+
  geom_line(aes(y = mean_estimate, color = "blue"))+
  labs(
    x = c("Size of Mu"),
    y = c("Mean of Estimate"),
    caption = c("Change in Mean of Total and Rejected Estimate based on Change in Mu Size")
  )

```

Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?
Based on the plot above, the sample average of μ̂ across tests for which the null is rejected is smaller than the true value of mu. As the null is rejected, compared to the total mean estimate, the sample average will be more deviated from the true value of mu. Therefore, in the case of this simulation, the sample average across tests for which the null is reject is smaller than the true value of mu.