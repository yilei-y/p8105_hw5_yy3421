---
title: "p8105_hw5_yy3421"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 10, 
  fig.height = 8,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

### Description of the raw data

```{r data import}
homicide_data_raw = read_csv("data/homicide-data.csv")
```

The raw dataset contains `r homicide_data_raw |> nrow()` observations and `r homicide_data_raw |> ncol()` variables.

```{r city_state var}
homicide_data = 
  homicide_data_raw |> 
  mutate(
    city_state = paste(city, state, sep = ", ")
  ) 
```

```{r total number of homicides}
total_homicides = 
  homicide_data |> 
  group_by(city_state) |> 
  summarise(total_number_of_homicides = n()) 
```

```{r number of unsolved homicide}
unsolved_homicide = 
  homicide_data |> 
  group_by(city_state) |> 
  filter(
    disposition %in% c("Closed without arrest", "Open/No arrest")
  ) |> 
  summarise(number_of_unsolved_homicides = n()) 
```

## Prop test for Baltimore
```{r prop test Baltimore}
output_Baltimore = vector("list", length = 3)

Baltimore_total = 
  total_homicides |> 
  filter(
    city_state == "Baltimore, MD"
  ) 

Baltimore_unsolved = 
  unsolved_homicide |> 
  filter(
    city_state == "Baltimore, MD"
  ) 
    
prop_Balt = prop.test(1825, 2827, alternative = "two.sided") |> broom::tidy()

```

## Problem 2
```{r data cleaning and tidying}
files_collection = 
  tibble(
    file_name = list.files(path = "./data/problem_2"),
    file_path = paste(c("./data/problem_2/"), file_name, sep = "")
  ) |>
  mutate(
    data = map(file_path, read_csv)
  ) |> 
  separate(file_name, into = c("control_arm", "subject_ID"), sep = "_") |> 
  separate(subject_ID, into = c("subject_ID", "file_type"), sep = "\\.")|> 
  unnest(data) |> 
  pivot_longer(
    week_1:week_8,
    names_to = c("week"),
    values_to = c("data") 
  ) |> 
  separate(week, into = c("week", "time"), sep = "_")|> 
  select(
    -file_type, -file_path, -week
  ) |> 
  mutate(
    time = as.numeric(time)
  )
```

Making plot:
```{r plot}
files_collection |> 
  ggplot(aes(x = time, y = data, color = subject_ID))+
  geom_line()+
  facet_grid(~control_arm)
```

### Comment on differences between groups
Based on the spaghetti plot above, we can oberve that there is a higher average data for the experiment arm. Furthermore, for all participants in the experiment arm, there is a increasing trend in data as we move from week 2 to 8. However, most data among all participants in the control arm remains in the interval 0 to 2.5.

## Problem 3

```{r function for sim}

output = vector("list", 5000)

sim_t_test = function(mu, n = 30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = sigma),
  )
  
  sim_data |> 
    t.test(conf.level = 0.95) |> 
    broom::tidy() |> 
    select(
      estimate, p.value
    )
    
}

```

### Simulation for mu = 0
```{r mu = 0}
for (i in 1:5000) {
  output[[i]] = sim_t_test(mu=0)
}

sim_results = bind_rows(output)
```

### Simulation for mu = 0:6
```{r mu simulation}
sim_results_df = 
  expand_grid(
    mu_size = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(mu_size, sim_t_test)
    )|> 
  unnest(estimate_df)
```

# the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis
```{r}
sim_results_df |> 
  group_by(mu_size) |> 
  filter(p.value >= 0.05) |> 
  summarize(n_reject = n()) |> 
  mutate(
    n_reject = n_reject / 5000
  ) |> 
  ggplot(aes(x = mu_size, y = n_reject, color = mu_size))+
  geom_point()+
  geom_line()
```

Describe the association between effect size and power.
Based on the plot above, there is a decreasing trend in the proportion of times the null was rejected as we increase the mu size from 0 to 5. There is 0 times the null was rejected when mu equals to 6. Hence, as the effect size increases, the power of this study increases.

#second plot
```{r plot 2}
sim_results_df_mean_mu=
  sim_results_df|> 
  group_by(mu_size) |> 
  summarise(mean_estimate = mean(estimate))

sim_results_df |> 
  group_by(mu_size) |> 
  filter(p.value >= 0.05) |> 
  summarize(mean_reject = mean(estimate)) |> 
  right_join(sim_results_df_mean_mu) |> 
  ggplot(aes(x = mu_size)) +
  geom_line(aes(y = mean_reject, color = mean_reject))+
  geom_line(aes(y = mean_estimate, color = mean_estimate))+
  labs(
    x = c("Size of Mu"),
    y = c("Mean of Estimate"),
    caption = c("Change in Mean of Total and Rejected Estimate based on Change in Mu Size")
  )

```

Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?
Based on the plot above, the sample average of μ̂ across tests for which the null is rejected is smaller than the true value of mu. As the null is rejected, compared to the total mean estimate, the sample average will be more deviated from the true value of mu. Therefore, in the case of this simulation, the sample average across tests for which the null is reject is smaller than the true value of mu.