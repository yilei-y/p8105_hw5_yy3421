---
title: "p8105_hw5_yy3421"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 10, 
  fig.height = 8,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

### Description of the raw data

```{r data import}
homicide_data_raw = read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown"))
```


```{r city_state var and cleaning}
homicide_data = 
  homicide_data_raw |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved")
  ) |> 
  filter(city_state != "Tulsa, AL") 
```

The raw dataset contains `r homicide_data_raw |> nrow()` observations and `r homicide_data_raw |> ncol()` variables. This dataset also contains demographic information of the victim for each case, like age, sex, and race, and case-specific information, like date, and location.

Through data cleaning, a `city_state` variable was first created to combine the information from city and state variable. Furthermore, a `resolution` variable was created to distinguish the final status of each case. One entry in Tulsa, AL was excluded through data cleaning as the location is not a major US city.

```{r total and unsolved cases}
city_homicide_df = 
  homicide_data %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```


The above code chunks summarizes the total number of homicides and the number of homicides that were solved.

### Prop test for Baltimore

Looking at cases located in Baltimore, MD, proportion test was used, complemented by `broom::tidy` function, to estimate the proportion and confidence level of unsolved homicides in that city. 

```{r Balt prop}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD")  |>  pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD")  |>  pull(hom_total)) 

broom::tidy(bmore_test)  |>  
  knitr::kable(digits = 3)
```

Furthermore, estimates and CIs for the proportion of unsolved homicides for each city are presented below using the `map` function.

```{r all city prop}
test_results = 
  city_homicide_df  |>  
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy))  |>  
  select(-prop_tests)  |>  
  unnest(tidy_tests)  |>  
  select(city_state, estimate, conf.low, conf.high)  |>  
  mutate(city_state = fct_reorder(city_state, estimate))
```

Using the data from the above data chunk, a point plot, along with error bar, is created to visualized the estimate of the proportion of unsolved homicides in each city. Chicago, IL has the highest estimate and Richmond, VA has the lowest.

```{r p1 plot}
test_results  |>  
  mutate(city_state = fct_reorder(city_state, estimate))  |>  
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Problem 2
```{r data cleaning and tidying}
files_collection = 
  tibble(
    file_name = list.files(path = "./data/problem_2"),
    file_path = paste(c("./data/problem_2/"), file_name, sep = "")
  ) |>
  mutate(
    data = map(file_path, read_csv)
  ) |> 
  separate(file_name, into = c("control_arm", "subject_ID"), sep = "_") |> 
  separate(subject_ID, into = c("subject_ID", "file_type"), sep = "\\.")|> 
  unnest(data) |> 
  pivot_longer(
    week_1:week_8,
    names_to = c("week"),
    values_to = c("data") 
  ) |> 
  separate(week, into = c("week", "time"), sep = "_")|> 
  select(
    -file_type, -file_path, -week
  ) |> 
  mutate(
    time = as.numeric(time)
  )
```

To import all the files, the `list.files` function was first used to collect the names of all files in this folder into a tibble. The path of each file is saved into this tibble based on each file name. Using the `file_path` variable and `map` function, files are imported. 

In the data cleaning process, subject ID and control/experiment arm was separated from the file name using the `separate` function. After unnesting data for each file, the `pivot_longer` function was used to create `time` variable in the data frame for future analysis on the change in data. At last, `time` variable is transformed into a numeric variable for quantatitive data analysis.

### Making plot:
```{r plot}
files_collection |> 
  ggplot(aes(x = time, y = data, color = subject_ID))+
  geom_line()+
  geom_point(alpha = .3)+
  facet_grid(~control_arm)+
  labs(
    x = c("Week"),
    y = c("Data"),
    caption = c("Week Versus Data")
  )
```

After data cleaning, the above data chunk was used to visualize the change in data for control and experiment group along week 1 to week 8.

### Comment on differences between groups
Based on the spaghetti plot above, we can oberve that there is a higher average data for the experiment arm. Furthermore, for all participants in the experiment arm, there is a increasing trend in data as we move from week 1 to 8. However, most data among all participants in the control arm remains in the interval -2.5 to 2.5. This could indicate a influence of treatment on the experimental group compared to the control group.

## Problem 3

Before starting the simulation, the `sim_t_test` function was first created that functions to perform a t-test based on a unfixed μ value and fixed n and sigma values. In this function, `rnorm` function was first used to create the data frame based on the new μ value. Then, `t.test` function was used, combining with `broom::tidy` function to calculate the estimation of the mean and p value.

```{r function for sim}
sim_t_test = function(mu, n = 30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = sigma),
  )
  
  sim_data |> 
    t.test(conf.level = 0.95) |> 
    broom::tidy() |> 
    select(
      estimate, p.value
    )
    
}
```

After creating the function, simulation was done for μ = 0. An output list was first created for storage of data from simulation afterwards. Then, simulation was done for 5000 times and collected in the output list. At last, `bind_rows` was used to collect all 5000 simulations in one tibble.

### Simulation for mu = 0
```{r mu = 0}
output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim_t_test(mu=0)
}

sim_results = bind_rows(output)

sim_results
```

### Simulation for mu = 0:6
```{r mu simulation}
sim_results_df = 
  expand_grid(
    mu_size = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(mu_size, sim_t_test)
    )|> 
  unnest(estimate_df)
```

The above data chunk simulates results for μ = 0 to 6 using the `map` function. After collecting data of estimates and p-value for μ = 0 to 6, data visualization could be performed below.

```{r}
sim_results_df |> 
  group_by(mu_size) |> 
  filter(p.value < 0.05) |> 
  summarize(n_reject = n()) |> 
  mutate(
    n_reject = n_reject / 5000
  ) |> 
  ggplot(aes(x = mu_size, y = n_reject, color = mu_size))+
  geom_point()+
  geom_line()+
  labs(
    x = "True value of Mu",
    y = "Proportion of times the null was rejected",
    caption = c("True value of Mu Versus Proportion of times the null was rejected")
  )
```

The above plot visualize the association between the proportion of times the null was rejected and the true value of μ. Based on the plot above, there is an increasing trend in the proportion of times the null was rejected as we increase the μ size from 0 to 6. There is 100% the null was rejected when μ equals to 6. Hence, as the effect size increases, the power of this study increases.

```{r plot 2}
sim_results_df_mean_mu=
  sim_results_df|> 
  group_by(mu_size) |> 
  summarise(mean_estimate = mean(estimate))

sim_results_df |> 
  group_by(mu_size) |> 
  filter(p.value < 0.05) |> 
  summarize(mean_reject = mean(estimate)) |> 
  right_join(sim_results_df_mean_mu) |> 
  ggplot(aes(x = mu_size)) +
  geom_line(aes(y = mean_reject, color = "red"))+
  geom_line(aes(y = mean_estimate, color = "blue"))+
  labs(
    x = c("True μ"),
    y = c("Mean of Estimate"),
    caption = c("Change in Mean of Total and Rejected Estimate based on Change in Mu Size")
  )

```

Based on the plot above, the sample average of μ̂ across tests for which the null is rejected is smaller than the true value of μ. As the null is rejected, compared to the total mean estimate of μ, the sample average will be more deviated from the true value of μ. Furthermore, as the effective size increases, leading high power of the test, the possibility of rejecting the null increases. This high possibility could also leads to a higher sample average compared to the true μ.